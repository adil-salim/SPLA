{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Submitted version\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import snap\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from sklearn.neighbors.kde import KernelDensity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Entropy(X):\n",
    "    \"\"\" estimate entropy of the density\n",
    "    of the sample of the X[i,:]\n",
    "    Input:\n",
    "        X: matrix. Each row represents an iid realization of the density\"\"\"\n",
    "    \n",
    "    kde = KernelDensity(kernel='gaussian').fit(X)\n",
    "    return np.mean(kde.score_samples(X))\n",
    "\n",
    "\n",
    "def prox_edge(theta,gamma,i,j):\n",
    "    x = theta.copy()\n",
    "    if np.abs(theta[i] - theta[j]) < 2 * gamma :\n",
    "        x[i] = np.mean([theta[i],theta[j]])\n",
    "        x[j] = np.mean([theta[i],theta[j]])\n",
    "    else:\n",
    "        x[i] -= gamma * np.sign(x[i] - x[j])\n",
    "        x[j] -= gamma * np.sign(x[j] - x[i])\n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "def quadratic_loss(y, x):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return 1. / 2. * np.sum((y-x)**2)\n",
    "\n",
    "\n",
    "def gradient(G):\n",
    "    \"\"\"\n",
    "    compute gradient of a graph\n",
    "    \"\"\"\n",
    "    N = G.GetNodes()\n",
    "    M = G.GetEdges()\n",
    "\n",
    "    grad = np.zeros((M, N))\n",
    "\n",
    "    for (i, e) in enumerate(G.Edges()):\n",
    "        s = e.GetSrcNId()\n",
    "        d = e.GetDstNId()\n",
    "        grad[i, s], grad[i, d] = -1. , 1.\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "def Seuillage_Dur(v,seuil = 1.0):\n",
    "    y = np.copy(v)\n",
    "    for i in range(len(v)):\n",
    "        if np.abs(v[i]) > seuil:\n",
    "            y[i] = np.sign(v[i])*seuil\n",
    "    return y    \n",
    "\n",
    "\n",
    "def U(theta,G,temp,lmbda):\n",
    "    return temp * 0.5 * np.linalg.norm(y - theta) **2 + temp * lmbda * TV(theta,G)\n",
    "    \n",
    "    \n",
    "\n",
    "def proxtv(y, grad, lmbda, it = 100, pas = 0.001):\n",
    "    M, N = grad.shape\n",
    "    u = np.zeros(M)  \n",
    "    for i in range(it):\n",
    "        u = u - 0.001 * (np.dot(grad,np.dot(grad.T,u)) - np.dot(grad,y))\n",
    "        u = Seuillage_Dur(u,lmbda)\n",
    "    return y - np.dot(grad.T,u)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "def PLA(y, n_iter, gamma, lmbda, grad, temp, itprox = 100, pasprox = 0.001, gaussian_noise=True):\n",
    "    \"\"\" ProxLA algorithm\n",
    "    Inputs:\n",
    "        n_iter: number of iterations\n",
    "        y: signal on the original graph\n",
    "        gamma: constant step size\n",
    "        lmbda: weight of regularization\n",
    "        grad : graph gradient matrix\n",
    "        gaussian_noise: if False passty algorithm is applied, else Passty-Langevin\n",
    "     Output:\n",
    "         samples\n",
    "    \"\"\"\n",
    "    \n",
    "    M, N = G.GetEdges(), G.GetMxNId() #number of edges and vertices\n",
    "\n",
    "    samples_theta = np.zeros((n_iter, N)) #iterates stacked by line\n",
    "    errs = np.zeros((n_iter, 3)) #One column per error type\n",
    "    \n",
    "    theta = y.copy()\n",
    "    \n",
    "    times = [0]*n_iter\n",
    "    t = 0.0 #times\n",
    "    theta_out = theta.copy()\n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        #samples\n",
    "        times[i] = t\n",
    "        samples_theta[i] = theta_out\n",
    "\n",
    "        #errors\n",
    "        err_quad = 1. / 2. * np.sum((y - theta)**2)\n",
    "        err_reg = lmbda  * np.sum(np.abs(np.dot(grad,theta)))\n",
    "\n",
    "        errs[i] = err_quad + err_reg, err_quad, err_reg\n",
    "        \n",
    "        #algo\n",
    "        std = np.sqrt(2 * gamma) * float(gaussian_noise)\n",
    "        top = time.time()\n",
    "        \n",
    "        theta = proxtv((theta + gamma*temp*y)/(1.0 + gamma*temp), grad, lmbda*gamma*temp / (1.0 + gamma*temp), itprox, pasprox) \n",
    "        theta_out = theta.copy()\n",
    "        theta += std * np.random.randn(N)  \n",
    "        \n",
    "        tip = time.time() - top\n",
    "        t += tip\n",
    "        \n",
    "    return samples_theta, errs, times\n",
    "\n",
    "\n",
    "def TV(x,G):\n",
    "    r = 0\n",
    "    for e in G.Edges():\n",
    "        r += np.abs(x[e.GetSrcNId()] - x[e.GetDstNId()])\n",
    "    return r\n",
    "\n",
    "\n",
    "def passty_langevin_sto(Y, n_it, gamma, lmbda, G, b, L, temp, par, gaussian_noise=True):\n",
    "    \"\"\" Stochastic_Passty_Algorithm\n",
    "    Inputs:\n",
    "        temp: temperature\n",
    "        n_iter: number of iterations\n",
    "        Y: Matrix of signals on the original graph\n",
    "        gamma: constant step size\n",
    "        lmbda: weight of regularization\n",
    "        b : minibatch size for the likelihood\n",
    "        L : minibatch size for the prior\n",
    "        grad : graph gradient matrix\n",
    "        l : TV estimation\n",
    "        gaussian_noise: if False passty algorithm is applied, else Passty-Langevin\n",
    "     Output:\n",
    "         samples\n",
    "    \"\"\"\n",
    "    V, E = G.GetMxNId(), G.GetEdges() #number of edges and vertices\n",
    "    D = Y.shape[0] #number of data\n",
    "\n",
    "    samples_theta = np.zeros((n_it, V)) #iterates stacked by line\n",
    "    errs = np.zeros((n_it, 3)) #One column per error type\n",
    "    \n",
    "    theta = np.mean(Y,axis = 0) #Start with the mean of observations\n",
    "    t = 0.0 #time\n",
    "    std = np.sqrt(2 * gamma) * float(gaussian_noise)\n",
    "    times = [0]*n_it\n",
    "    theta_out = theta.copy()\n",
    "    for i in range(n_it):\n",
    "        if i % par == 0: \n",
    "            print i \n",
    "        times[i] = t\n",
    "        samples_theta[i] = theta_out\n",
    "        #Algo\n",
    "        \n",
    "        top = time.time() \n",
    "        for j in range(L):\n",
    "            #print j\n",
    "            e = G.GetRndEId()\n",
    "            e = G.GetEI(e)\n",
    "            theta = prox_edge(theta, temp * lmbda * gamma * E/float(L), e.GetSrcNId(), e.GetDstNId())\n",
    "        theta_out = theta.copy()\n",
    "        theta -= temp * gamma * np.mean([theta - Y[np.random.randint(D),:] for j in range(b)])\n",
    "        theta += std * np.random.randn(V)\n",
    "        tip = time.time() - top\n",
    "        t += tip\n",
    "    return samples_theta, times, errs\n",
    "\n",
    "def subgrad_langevin_sto(Y, n_it, gamma, lmbda, G, b, L, temp, par, gaussian_noise=True):\n",
    "    \"\"\" Stochastic_Passty_Algorithm\n",
    "    Inputs:\n",
    "        n_iter: number of iterations\n",
    "        Y: Matrix of signals on the original graph\n",
    "        gamma: constant step size\n",
    "        lmbda: weight of regularization\n",
    "        b : minibatch size for the likelihood\n",
    "        L : minibatch size for the prior\n",
    "        grad : graph gradient matrix\n",
    "        l : TV estimation\n",
    "        gaussian_noise: if False passty algorithm is applied, else Passty-Langevin\n",
    "     Output:\n",
    "         samples\n",
    "    \"\"\"\n",
    "    V, E = G.GetMxNId(), G.GetEdges() #number of edges and vertices\n",
    "    D = Y.shape[0] #number of data\n",
    "\n",
    "    samples_theta = np.zeros((n_it, V)) #iterates stacked by line\n",
    "    errs = np.zeros((n_it, 3)) #One column per error type\n",
    "    \n",
    "    theta = np.mean(Y, axis = 0) #Start with the mean of observations\n",
    "    t = 0.0 #time\n",
    "    std = np.sqrt(2 * gamma / E) * float(gaussian_noise)\n",
    "    times = [0]*n_it\n",
    "    for i in range(n_it):\n",
    "        if i % par == 0: \n",
    "            print i\n",
    "        times[i] = t\n",
    "        samples_theta[i] = theta\n",
    "        \n",
    "        top = time.time() \n",
    "        for j in range(L):\n",
    "            e = G.GetRndEId()\n",
    "            e = G.GetEI(e)\n",
    "            theta[e.GetSrcNId()] -= temp * lmbda * gamma * E/float(L) * np.sign(theta[e.GetSrcNId()] - theta[e.GetDstNId()])\n",
    "            theta[e.GetDstNId()] -= temp * lmbda * gamma * E/float(L) * np.sign(theta[e.GetDstNId()] - theta[e.GetSrcNId()])\n",
    "        theta -= temp * gamma * np.mean([theta - Y[np.random.randint(D),:] for j in range(b)])\n",
    "        theta += std * np.random.randn(V)\n",
    "        tip = time.time() - top\n",
    "        t += tip\n",
    "    return samples_theta, times, errs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#General parameters\n",
    "\n",
    "D = 1\n",
    "b = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Figure 1, FB gaussian\n",
    "\n",
    "\n",
    "G = snap.LoadEdgeList(snap.PNEANet, \"facebook_combined.txt\", 0, 1)\n",
    "N = G.GetNodes()\n",
    "M = G.GetEdges()\n",
    "print N,M\n",
    "\n",
    "lmbda = N * np.sqrt(np.pi) / (2*M)\n",
    "print lmbda * TV(np.random.randn(N),G)/(0.5 * np.linalg.norm(np.random.randn(N) - np.random.randn(N))**2) #Same weight\n",
    "Y = np.random.randn(D,N)\n",
    "Y.shape\n",
    "\n",
    "alpha = 50 * float(M) \n",
    "gamma = .1 * float(N) / alpha \n",
    "temp = alpha/float(M) \n",
    "L = 400 \n",
    "grad = gradient(G)\n",
    "par_spla = 100\n",
    "par_ssla = par_spla * L / 80\n",
    "\n",
    "\n",
    "n_it = 1500\n",
    "nb_samples = 5\n",
    "n_it_sub = n_it * L / 80\n",
    "\n",
    "gamma_prox = .5 / temp\n",
    "n_iter_pla = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Figure 2, FB inpainting\n",
    "\n",
    "\n",
    "G = snap.LoadEdgeList(snap.PNEANet, \"facebook_combined.txt\", 0, 1)\n",
    "N = G.GetNodes()\n",
    "M = G.GetEdges()\n",
    "print N,M\n",
    "\n",
    "lmbda = N * np.sqrt(np.pi) / (2*M)\n",
    "print lmbda * TV(np.random.randn(N),G)/(0.5 * np.linalg.norm(np.random.randn(N) - np.random.randn(N))**2) #Same weight\n",
    "\n",
    "Y = np.random.randn(D,N)\n",
    "Y.shape\n",
    "\n",
    "for i in range(N/2):\n",
    "    Y[0,i] = 0.0\n",
    "\n",
    "alpha = 50 * float(M) \n",
    "gamma = .1 * float(N) / alpha \n",
    "temp = alpha/float(M) \n",
    "L = 400 \n",
    "\n",
    "grad = gradient(G)\n",
    "\n",
    "par_spla = 100\n",
    "\n",
    "par_ssla = par_spla * L / 80\n",
    "\n",
    "\n",
    "n_it = 1500\n",
    "nb_samples = 5\n",
    "\n",
    "n_it_sub = n_it * L / 80\n",
    "\n",
    "\n",
    "gamma_prox = .5 / temp\n",
    "n_iter_pla = 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Figure 3, Youtube gaussian\n",
    "\n",
    "\n",
    "G = snap.LoadEdgeList(snap.PNEANet, \"com-youtube.ungraph.txt\", 0, 1)\n",
    "N = G.GetMxNId()\n",
    "M = G.GetEdges()\n",
    "print N,M\n",
    "\n",
    "lmbda = N * np.sqrt(np.pi) / (2*M)\n",
    "print lmbda * TV(np.random.randn(N),G)/(0.5 * np.linalg.norm(np.random.randn(N) - np.random.randn(N))**2) #Same weight\n",
    "\n",
    "Y = np.random.randn(D,N)\n",
    "Y.shape\n",
    "\n",
    "\n",
    "\n",
    "alpha = 50 * float(M) \n",
    "gamma = .001 * float(N) / alpha \n",
    "temp = alpha/float(M) \n",
    "L = 400 \n",
    "\n",
    "par_spla = 1\n",
    "\n",
    "\n",
    "par_ssla = par_spla * L / 20\n",
    "\n",
    "\n",
    "n_it = 15 \n",
    "nb_samples = 5\n",
    "\n",
    "n_it_sub = n_it * L / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Figure 4, Youtube inpainting\n",
    "\n",
    "\n",
    "G = snap.LoadEdgeList(snap.PNEANet, \"com-youtube.ungraph.txt\", 0, 1)\n",
    "N = G.GetMxNId()\n",
    "M = G.GetEdges()\n",
    "print N,M\n",
    "\n",
    "lmbda = N * np.sqrt(np.pi) / (2*M)\n",
    "print lmbda * TV(np.random.randn(N),G)/(0.5 * np.linalg.norm(np.random.randn(N) - np.random.randn(N))**2) #Same weight\n",
    "\n",
    "Y = np.random.randn(D,N)\n",
    "Y.shape\n",
    "\n",
    "\n",
    "for i in range(N/2):\n",
    "    Y[0,i] = 0.0\n",
    "\n",
    "alpha = 50 * float(M) \n",
    "gamma = .001 * float(N) / alpha \n",
    "temp = alpha/float(M) \n",
    "L = 4000 \n",
    "\n",
    "par_spla = 1\n",
    "\n",
    "\n",
    "par_ssla = par_spla * L / 80\n",
    "\n",
    "\n",
    "n_it = 15\n",
    "nb_samples = 5\n",
    "\n",
    "n_it_sub = n_it * 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Figure 5-6, DBLP\n",
    "\n",
    "\n",
    "G = snap.LoadEdgeList(snap.PNEANet, \"com-dblp.ungraph.txt\", 0, 1)\n",
    "N = G.GetMxNId()\n",
    "M = G.GetEdges()\n",
    "print N,M\n",
    "\n",
    "lmbda = N * np.sqrt(np.pi) / (2*M)\n",
    "print lmbda * TV(np.random.randn(N),G)/(0.5 * np.linalg.norm(np.random.randn(N) - np.random.randn(N))**2) #Same weight\n",
    "\n",
    "Y = np.random.randn(D,N)\n",
    "Y.shape\n",
    "\n",
    "alpha = 50 * float(M) \n",
    "gamma = .01 * float(N) / alpha \n",
    "temp = alpha/float(M) \n",
    "L = 400\n",
    "\n",
    "par_spla = 1\n",
    "\n",
    "\n",
    "par_ssla = par_spla * L / 80\n",
    "\n",
    "\n",
    "n_it = 15\n",
    "nb_samples = 5\n",
    "\n",
    "n_it_sub = n_it * L / 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Figure 7-8, Amazon\n",
    "\n",
    "\n",
    "G = snap.LoadEdgeList(snap.PNEANet, \"com-amazon.ungraph.txt\", 0, 1)\n",
    "N = G.GetMxNId()\n",
    "M = G.GetEdges()\n",
    "print N,M\n",
    "\n",
    "lmbda = N * np.sqrt(np.pi) / (2*M)\n",
    "print lmbda * TV(np.random.randn(N),G)/(0.5 * np.linalg.norm(np.random.randn(N) - np.random.randn(N))**2) #Same weight\n",
    "\n",
    "Y = np.random.randn(D,N)\n",
    "Y.shape\n",
    "\n",
    "alpha = 50 * float(M) \n",
    "gamma = .01 * float(N) / alpha \n",
    "temp = alpha/float(M) \n",
    "L = 400\n",
    "\n",
    "par_spla = 1\n",
    "\n",
    "\n",
    "par_ssla = par_spla * L / 80\n",
    "\n",
    "\n",
    "n_it = 15\n",
    "nb_samples = 5\n",
    "\n",
    "n_it_sub = n_it * L / 80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "samples = []\n",
    "times_mean = []\n",
    "\n",
    "\n",
    "for i in range(nb_samples):\n",
    "    thetas, times, errs = passty_langevin_sto(Y, n_it, gamma, lmbda, G, b, L, temp, par_spla, gaussian_noise=True)\n",
    "    samples += [thetas]\n",
    "    times_mean += [times]\n",
    "    \n",
    "print time.time() - t0\n",
    "print times[-1]\n",
    "\n",
    "y = Y[0,:]\n",
    "\n",
    "#Minimization KL\n",
    "samples_mix = []\n",
    "for i in range(nb_samples):\n",
    "    thetas_mix = []\n",
    "    for j in range(n_it):\n",
    "        k = np.random.randint(j+1)\n",
    "        thetas_mix += [samples[i][k,:]] #mixing\n",
    "    samples_mix += [thetas_mix]\n",
    "\n",
    "    \n",
    "A = np.asarray(times_mean)\n",
    "t_pass = np.mean(A,axis=0)\n",
    "t_pass = list(t_pass)    \n",
    "\n",
    "#KL estimation\n",
    "\n",
    "\n",
    "KL = []\n",
    "time_passty = []\n",
    "for k in range(n_it):\n",
    "    if k % par_spla == 0: \n",
    "        print k\n",
    "        time_passty += [t_pass[k]]\n",
    "        Theta_k = np.asarray([samples_mix[i][k] for i in range(nb_samples)]) #each line is a realization of SPLA after mixing\n",
    "        H_k = Entropy(Theta_k) \n",
    "        E_k = np.mean([U(Theta_k[i,:],G,temp,lmbda) for i in range(nb_samples)])\n",
    "        KL += [H_k + E_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We need a small step size for this method to work\n",
    "t0 = time.time()\n",
    "samples_sub = []\n",
    "times_mean_sub = []\n",
    "\n",
    "for i in range(nb_samples):\n",
    "    thetas, times, errs = subgrad_langevin_sto(Y, n_it_sub, gamma, lmbda, G, b, L, temp, par_ssla, gaussian_noise=True)\n",
    "    samples_sub += [thetas]\n",
    "    times_mean_sub += [times]\n",
    "    \n",
    "print time.time() - t0\n",
    "print times[-1]\n",
    "\n",
    "y = Y[0,:]\n",
    "\n",
    "samples_mix_sub = []\n",
    "for i in range(nb_samples):\n",
    "    thetas_mix_sub = []\n",
    "    for j in range(n_it_sub):\n",
    "        k = np.random.randint(j+1)\n",
    "        thetas_mix_sub += [samples_sub[i][k,:]] \n",
    "    samples_mix_sub += [thetas_mix_sub]\n",
    "\n",
    "\n",
    "B = np.asarray(times_mean_sub)\n",
    "t_sub = np.mean(B,axis=0)\n",
    "t_sub = list(t_sub)\n",
    "\n",
    "time_sub = []\n",
    "KL_sub = []\n",
    "for k in range(n_it_sub):\n",
    "    if k % par_ssla == 0:\n",
    "        print k\n",
    "        time_sub += [t_sub[k]]\n",
    "        Theta_k = np.asarray([samples_mix_sub[i][k] for i in range(nb_samples)]) \n",
    "        H_k = Entropy(Theta_k) \n",
    "        E_k = np.mean([U(Theta_k[i,:],G,temp,lmbda) for i in range(nb_samples)])\n",
    "        KL_sub += [H_k + E_k]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ProxLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = Y[0,:]\n",
    "t0 = time.time()\n",
    "samples_pla = []\n",
    "times_mean_pla = []\n",
    "\n",
    "for i in range(nb_samples):\n",
    "    thetas_pla, errs_pla, times_pla = PLA(y, n_iter_pla, gamma_prox, lmbda, temp, grad, itprox = 10, pasprox = 0.001, gaussian_noise=True)\n",
    "    samples_pla += [thetas_pla]\n",
    "    times_mean_pla += [times_pla]\n",
    "    \n",
    "print time.time() - t0\n",
    "\n",
    "samples_mix_pla = []\n",
    "for i in range(nb_samples):\n",
    "    thetas_mix_pla = []\n",
    "    for j in range(n_iter_pla):\n",
    "        k = np.random.randint(j+1)\n",
    "        thetas_mix_pla += [samples_pla[i][k,:]] \n",
    "    samples_mix_pla += [thetas_mix_pla]\n",
    "\n",
    "\n",
    "C = np.asarray(times_mean_pla)\n",
    "t_pla = np.mean(C,axis=0)\n",
    "t_pla = list(t_pla)\n",
    "\n",
    "\n",
    "KL_pla = []\n",
    "\n",
    "for k in range(n_iter_pla):\n",
    "    Theta_k = np.asarray([samples_mix_pla[i][k] for i in range(nb_samples)]) \n",
    "    H_k = Entropy(Theta_k)\n",
    "    E_k = np.mean([U(Theta_k[i,:],G,temp,lmbda) for i in range(nb_samples)])\n",
    "    KL_pla += [H_k + E_k]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "p1 = round(1/temp,2)\n",
    "p2 = round(temp * lmbda,1)\n",
    "p3 = round(gamma,6)\n",
    "#p4 = round(gamma_prox,2)\n",
    "\n",
    "passty_time = plt.plot(time_passty, KL, drawstyle='Default',marker = 'None', label = 'SPLA, $\\gamma$ = '+ str(p3))\n",
    "#pla_time = plt.plot(t_pla, KL_pla, drawstyle='steps-post',marker = 's', label = 'ProxLA, $\\gamma$ = '+ str(p4))\n",
    "sub_time = plt.plot(time_sub, KL_sub, drawstyle='default',marker = 'None', label = 'SSLA, $\\gamma$ = '+ str(p3), linestyle = '--')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('CPU time (in seconds)')\n",
    "plt.ylabel('$E+H$')\n",
    "plt.title('$E+H$ as a function of time, $\\sigma^2$ = '+str(p1)+' $, \\lambda$ = '+str(p2))\n",
    "fontP = FontProperties()\n",
    "legend = plt.legend(loc = 0, shadow = True, fontsize = 'x-large')\n",
    "legend.get_frame().set_facecolor('#efe2c1')\n",
    "plt.axis('tight')\n",
    "plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "plt.grid()\n",
    "#plt.savefig('Langevin-Fig3-v2.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
